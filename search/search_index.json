{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the Dynamic Map Benchmark Wiki Page!</p> <p>You can always press <code>F</code> or top right search bar to search for specific topics. </p> <p>Please give us a star\ud83c\udf1f and cite our work\ud83d\udcd6 to support our work if you find this useful.</p> CHANGELOG: <ul> <li>2024/08/24: Reorganize the wiki page. Add all scripts usage in the benchmark code, Data Overview with visualization, Method Overview with a demo to output clean map and Evaluation to evaluate the performance of the methods and also automatically output the visualization in the paper.</li> <li>2024/06/25: Qingwen is starting to work on the wiki page.  </li> <li>2024/04/29 BeautyMap is accepted by RA-L'24. Updated benchmark: BeautyMap and DeFlow submodule instruction in the benchmark. Added the first data-driven method DeFlow into our benchmark. Feel free to check.</li> <li>2024/04/18 DUFOMap is accepted by RA-L'24. Updated benchmark: DUFOMap and dynablox submodule instruction in the benchmark. Two datasets w/o gt for demo are added in the download link. Feel free to check.</li> <li>2024/03/08 Fix statements on our ITSC'23 paper: KITTI sequences pose are also from SemanticKITTI which used SuMa. In the DUFOMap paper Section V-C, Table III, we present the dynamic removal result on different pose sources. Check discussion in DUFOMap paper if you are interested.</li> <li>2023/06/13 The benchmark paper Accepted by ITSC 2023 and release five methods (Octomap, Octomap w GF, ERASOR, Removert) and three datasets (01, 05, av2, semindoor) in benchmark paper.</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Task: Detect and Remove dynamic points from Point Cloud Maps.</p> <p></p> <p>Here is a figure that illustrate of ghost points resulting from dynamic objects in KITTI sequence 7.  The yellow points to the right represent points labeled as belonging to dynamic objects in the dataset.  These ghost points negatively affect downstream tasks and overall point cloud quality</p>"},{"location":"#good-to-start-from-here","title":"\ud83c\udfd8\ufe0f Good to start from here","text":"<ul> <li> <p>What kind of data format we use? </p> <p>PCD files (pose information saved in <code>VIEWPOINT</code> header). Read Data Section</p> </li> <li> <p>How to evaluate the performance of a method?</p> <p>Two python scripts. Read Evaluation Section</p> </li> <li> <p>How to run a benchmark method on my data?</p> <p>Format your data and run the method. Read Create data and Run method</p> </li> </ul>"},{"location":"#methods-we-included","title":"\ud83c\udf81 Methods we included","text":"<p>Online (w/o prior map):</p> <ul> <li> DUFOMap (Ours \ud83d\ude80): RAL'24, Benchmark Instruction</li> <li> Octomap w GF (Ours \ud83d\ude80): ITSC'23, Benchmark improvement ITSC 2023</li> <li> dynablox: RAL'23 official link, Benchmark Adaptation </li> <li> Octomap: ICRA'10 &amp; AR'13 official link, Benchmark implementation</li> </ul> <p>Learning-based (data-driven) (w pretrain-weights provided):</p> <ul> <li> DeFlow (Ours \ud83d\ude80): ICRA'24, Benchmark Adaptation</li> </ul> <p>Offline (need prior map).</p> <ul> <li> BeautyMap (Ours \ud83d\ude80): RAL'24, Official Code</li> <li> ERASOR: RAL'21 official link, benchmark implementation</li> <li> Removert: IROS 2020 official link, benchmark implementation</li> </ul> <p>Please note that we provided the comparison methods also but modified a little bit for us to run the experiments quickly, but no modified on their methods' core. Please check the LICENSE of each method in their official link before using it.</p> <p>You will find all methods in this benchmark under <code>methods</code> folder. So that you can easily reproduce the experiments. Or click here to check our score screenshot directly. </p> <p>Last but not least, feel free to pull request if you want to add more methods. Welcome!</p>"},{"location":"#acknowledgements","title":"\ud83d\udc96 Acknowledgements","text":"<p>This benchmark implementation is based on codes from several repositories as we mentioned in the beginning. Thanks for these authors who kindly open-sourcing their work to the community. Please see our paper reference section to get more information.</p> <p>Thanks to HKUST Ramlab's members: Bowen Yang, Lu Gan, Mingkai Tang, and Yingbing Chen, who help collect additional datasets.</p> <p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation</p>"},{"location":"#cite-our-papers","title":"Cite Our Papers","text":"<p>Please cite our works if you find these useful for your research:</p> <pre><code>@inproceedings{zhang2023benchmark,\n  author={Zhang, Qingwen and Duberg, Daniel and Geng, Ruoyu and Jia, Mingkai and Wang, Lujia and Jensfelt, Patric},\n  booktitle={IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, \n  title={A Dynamic Points Removal Benchmark in Point Cloud Maps}, \n  year={2023},\n  pages={608-614},\n  doi={10.1109/ITSC57777.2023.10422094}\n}\n@article{daniel2024dufomap,\n  author={Duberg, Daniel and Zhang, Qingwen and Jia, Mingkai and Jensfelt, Patric},\n  journal={IEEE Robotics and Automation Letters}, \n  title={{DUFOMap}: Efficient Dynamic Awareness Mapping}, \n  year={2024},\n  volume={9},\n  number={6},\n  pages={5038-5045},\n  doi={10.1109/LRA.2024.3387658}\n}\n@article{jia2024beautymap,\n  author={Jia, Mingkai and Zhang, Qingwen and Yang, Bowen and Wu, Jin and Liu, Ming and Jensfelt, Patric},\n  journal={IEEE Robotics and Automation Letters}, \n  title={{BeautyMap}: Binary-Encoded Adaptable Ground Matrix for Dynamic Points Removal in Global Maps}, \n  year={2024},\n  volume={9},\n  number={7},\n  pages={6256-6263},\n  doi={10.1109/LRA.2024.3402625}\n}\n</code></pre>"},{"location":"papers/","title":"Project Lists","text":""},{"location":"papers/#lists","title":"Lists","text":"<p>Papers that use this benchmark with code open-sourced are list here. Sorted by the publication year. Please feel free to add your paper here by pull request. (Click the <code>Edit</code> button on the top right of this page)</p> <ul> <li> <p> [CVPR'24] 3D LiDAR Mapping in Dynamic Environments using a 4D Implicit Neural Representation, [pdflink] </p> </li> <li> <p> [RA-L'24]: BeautyMap: Dynamic Fusion Occupancy Map for Dynamic Scene Understanding, [2405.07283] </p> </li> <li> <p> [RA-L'24]: DUFOMap: Dynamic Fusion Occupancy Map for Dynamic Scene Understanding, [2403.01449] </p> </li> <li> <p> [ITSC'23]: A Dynamic Points Removal Benchmark in Point Cloud Maps, [2307.07260] </p> </li> </ul>"},{"location":"data/","title":"Data Overview","text":""},{"location":"data/#data-description","title":"Data Description","text":"<p>In this section, we will introduce the data format we use in the benchmark, and how to visualize the data easily. Next section on creation will show you how to create this format data from your own data.</p>"},{"location":"data/#benchmark-unified-format","title":"Benchmark Unified Format","text":"<p>We saved all our data into PCD files, first let me introduce the PCD file format:</p> <p>The important two for us are <code>VIEWPOINT</code>, <code>POINTS</code> and <code>DATA</code>:</p> <ul> <li> <p>VIEWPOINT - specifies an acquisition viewpoint for the points in the dataset. This could potentially be later on used for building transforms between different coordinate systems, or for aiding with features such as surface normals, that need a consistent orientation.</p> <p>The viewpoint information is specified as a translation (tx ty tz) + quaternion (qw qx qy qz). The default value is:</p> <pre><code>VIEWPOINT 0 0 0 1 0 0 0\n</code></pre> </li> <li> <p>POINTS - specifies the number of points in the dataset.</p> </li> <li> <p>DATA - specifies the data type that the point cloud data is stored in. As of version 0.7, three data types are supported: ascii, binary, and binary_compressed. We saved as binary for faster reading and writing.</p> </li> </ul>"},{"location":"data/#a-header-example","title":"A Header Example","text":"<p>I directly show a example header here from <code>004390.pcd</code> in KITTI sequence 00:</p> <pre><code># .PCD v0.7 - Point Cloud Data file format\nVERSION 0.7\nFIELDS x y z intensity\nSIZE 4 4 4 4\nTYPE F F F F\nCOUNT 1 1 1 1\nWIDTH 125883\nHEIGHT 1\nVIEWPOINT -15.6504 17.981 -0.934952 0.882959 -0.0239536 -0.0058903 -0.468802\nPOINTS 125883\nDATA binary\n</code></pre> <p>In this <code>004390.pcd</code> we have 125883 points, and the pose (sensor center) of this frame is: <code>-15.6504 17.981 -0.934952 0.882959 -0.0239536 -0.0058903 -0.468802</code>. </p> <p>Again, all points from data frames are already transformed to the world frame and VIEWPOINT is the sensor pose.</p>"},{"location":"data/#how-to-read-pcd-files","title":"How to read PCD files","text":"<p>In C++, we usually use PCL library to read PCD files, here is a simple example:</p> <pre><code>#include &lt;pcl/point_cloud.h&gt;\n#include &lt;pcl/point_types.h&gt;\n\npcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr pcd(new pcl::PointCloud&lt;pcl::PointXYZI&gt;);\npcl::io::loadPCDFile&lt;pcl::PointXYZI&gt;(\"data/00/004390.pcd\", *pcd);\n</code></pre> <p>In Python, we have a simple script to read PCD files in the benchmark code, or from my gits, you don't need to read the script in detail but use it directly.</p> <pre><code>import pcdpy3 # the script I provided\npcd_data = pcdpy3.PointCloud.from_path('data/00/004390.pcd')\npc = pcd_data.np_data[:,:3] # shape (N, 3) N: the number of point, 3: x y z\n# if the header have intensity or rgb field, you can get it by:\n# pc_intensity = pcd_data.np_data[:,3] # shape (N,)\n# pc_rgb = pcd_data.np_data[:,3:6] # shape (N, 3)\n</code></pre>"},{"location":"data/#download-benchmark-data","title":"Download benchmark data","text":"<p>We already processed the data in the benchmark, you can download the data from the following links:</p> Dataset Description Sensor Type Total Frame Number Size KITTI sequence 00 in a small town with few dynamics (including one pedestrian around) VLP-64 141 384.8 MB KITTI sequence 05 in a small town straight way, one higher car, the benchmarking paper cover image from this sequence. VLP-64 321 864.0 MB Argoverse2 in a big city, crowded and tall buildings (including cyclists, vehicles, people walking near the building etc. 2 x VLP-32 575 1.3 GB KTH campus (no gt) Collected by us (Thien-Minh) on the KTH campus. Lots of people move around on the campus. Leica RTC360 18 256.4 MB Semi-indoor Collected by us, running on a small 1x2 vehicle with two people walking around the platform. VLP-16 960 620.8 MB Twofloor (no gt) Collected by us (Bowen Yang) in a quadruped robot. A two-floor structure environment with one pedestrian around. Livox-mid 360 3305 725.1 MB <p>Download command: </p><pre><code>wget https://zenodo.org/api/records/10886629/files-archive.zip\n\n# or download each sequence separately\nwget https://zenodo.org/records/10886629/files/00.zip\nwget https://zenodo.org/records/10886629/files/05.zip\nwget https://zenodo.org/records/10886629/files/av2.zip\nwget https://zenodo.org/records/10886629/files/kthcampus.zip\nwget https://zenodo.org/records/10886629/files/semindoor.zip\nwget https://zenodo.org/records/10886629/files/twofloor.zip\n</code></pre>"},{"location":"data/#visualize-the-data","title":"Visualize the data","text":"<p>We provide a simple script to visualize the data in the benchmark, you can find it in scripts/py/data/play_data.py. You may want to download the data and requirements first.</p> <pre><code>cd scripts/py\n\n# download the data\nwget https://zenodo.org/records/10886629/files/twofloor.zip\n\n# https://github.com/KTH-RPL/DynamicMap_Benchmark/blob/main/scripts/py/requirements.txt\npip install -r requirements.txt\n</code></pre> <p>Run it: </p><pre><code>python data/play_data.py --data_folder /home/kin/data/twofloor --speed 1 # speed 1 for normal speed, 2 for faster with 2x speed\n</code></pre> <p>It will pop up a window to show the point cloud data, you can use the mouse to rotate, zoom in/out, and move the view. Terminal show the help information to start/stop the play.</p> <p></p> <p>The axis here shows the sensor frame. The video is play in sensor-frame, so you can see the sensor move around in the video.</p>"},{"location":"data/creation/","title":"Data Creation","text":""},{"location":"data/creation/#data-creation","title":"Data Creation","text":"<p>In this section, we demonstrate how to extract expected format data from public datasets (KITTI, Argoverse 2) and also collected by ourselves (rosbag). </p> <p>Still, I recommend you to download the benchmark data directly from the Zenodo link without reading this section. Back to data download and visualize page. It's only needed for people who want to run more data from themselves.</p>"},{"location":"data/creation/#create-by-yourself","title":"Create by yourself","text":"<p>If you want to process more data, you can follow the instructions below. </p> <p>Note</p> <p>Feel free to skip this section if you only want to use the benchmark data.    </p>"},{"location":"data/creation/#custom-data","title":"Custom Data","text":"<p>For our custom dataset, we normally record the pointcloud with rosbag, and then running some slam methods to get the pose. If you don't have clue to use the slam package, check simple_ndt_slam repo the only dependence you need in the repo is ROS. If you don't have ROS/Ubuntu, you can directly use the <code>docker</code> to run.</p> <p>Then, directly export rosbag file [which have pose/tf and pointcloud topic] to pcd we want, after your run with <code>simple_ndt_slam</code> check your result rosbag file by <code>rosbag info</code>, here is example output:</p> <pre><code>\u279c  bags rosbag info res_semi_indoor_data.bag\npath:        res_semi_indoor_data.bag\nversion:     2.0\nduration:    1:47s (107s)\nstart:       Apr 28 2023 11:11:26.79 (1682673086.79)\nend:         Apr 28 2023 11:13:14.35 (1682673194.35)\nsize:        810.8 MB\nmessages:    4803\ncompression: none [961/961 chunks]\ntypes:       nav_msgs/Odometry       [cd5e73d190d741a2f92e81eda573aca7]\n             sensor_msgs/PointCloud2 [1158d486dd51d683ce2f1be655c3c181]\n             tf2_msgs/TFMessage      [94810edda583a504dfda3829e70d7eec]\ntopics:      /auto_odom       960 msgs    : nav_msgs/Odometry      \n             /repub_points    960 msgs    : sensor_msgs/PointCloud2\n             /tf             2883 msgs    : tf2_msgs/TFMessage\n</code></pre> <p>Then use the scripts I provided in <code>simple_ndt_slam</code> to extract the pcd data to unified format here.</p> <pre><code>roscore # since need read rosbag through scripts\n\n./simple_ndt_slam/tools/build/bag2pcd_tf /home/kin/bags/res_semi_indoor_data.bag /home/kin/data/semindoor /repub_points map 1 # 1 for save raw map also since some methods need use it in the framework.\n</code></pre>"},{"location":"data/creation/#kitti-dataset","title":"KITTI Dataset","text":"<p>Official data format Download link</p>"},{"location":"data/creation/#extract-point-cloud-data","title":"Extract Point Cloud Data","text":"<p>extract the semantic-kitti dataset from the raw data, when you download the original SemanticKITTI dataset, you will get a folder like this: </p><pre><code>\u279c  SemanticKitti tree -L 2\n.\n\u251c\u2500\u2500 data_odometry_calib\n\u2502   \u2514\u2500\u2500 dataset\n\u2502       \u2514\u2500\u2500 sequences\n\u251c\u2500\u2500 data_odometry_labels\n\u2502   \u251c\u2500\u2500 dataset\n\u2502   \u2502   \u2514\u2500\u2500 sequences\n\u2502   \u2514\u2500\u2500 README\n\u251c\u2500\u2500 data_odometry_velodyne\n\u2502   \u2514\u2500\u2500 dataset\n\u2502       \u2514\u2500\u2500 sequences\n</code></pre> <p>After downloading the official dataset, Run the script like follows: </p><pre><code>python3 scripts/data/extract_semkitti.py --original_path /home/kin/data/KITTI/SemanticKitti --save_data_folder /home/kin/data/DynamicMap --gt_cloud True --sequence \"'00'\" \n</code></pre> <p>Note!! </p> <ol> <li> <p>SemanticKITTI pose file is not ground truth pose but run SuMa, more discussion and different can be found here in semantic-kitti-api/issues/140. We have extra different odometry pose result in DUFOMap paper, Sec V-C, Table III, based on scripts/py/data/extract_diff_pose.py</p> </li> <li> <p>You can get the sensor pose in the PCD <code>VIEWPOINT</code> Field, so you don't need pose file etc.     If you are using CloudCompare to view, drag all pcd files to windows, you will have the correct whole map view.     (NOTE Since we already transform to world frame CloudCompare in 2.11 version will looks correct map     but version to 2.12+ will have double effect on VIEWPOINT Field [you can comment the transform line if you don't like that.] )</p> <p>Example here: </p> </li> <li> <p>View the ground truth in CloudCompare, intensity=1 means dynamic which are red points in images:</p> <p></p> </li> <li> <p>2024/03/27 Updated version limit the range because we find the ground truth label is not correct in the far range, so we limit the range to 50m. You can change the range in the script.</p> <p></p> </li> </ol>"},{"location":"data/creation/#argoverse-20-dataset","title":"Argoverse 2.0 Dataset","text":"<p>I manually labeled dynamic and static in one sequence folder name: <code>07YOTznatmYypvQYpzviEcU3yGPsyaGg__Spring_2020</code> , you have to go for website to downloaded this ground truth PCD.</p>"},{"location":"data/creation/#download","title":"Download","text":"<p>Check this issue: https://github.com/argoverse/av2-api/issues/161</p> <p>Installing s5cmd</p> <pre><code>#!/usr/bin/env bash\n\nexport INSTALL_DIR=$HOME/.local/bin\nexport PATH=$PATH:$INSTALL_DIR\nexport S5CMD_URI=https://github.com/peak/s5cmd/releases/download/v1.4.0/s5cmd_1.4.0_$(uname | sed 's/Darwin/macOS/g')-64bit.tar.gz\n\nmkdir -p $INSTALL_DIR\ncurl -sL $S5CMD_URI | tar -C $INSTALL_DIR -xvzf - s5cmd\n</code></pre> <p>Download the val dataset since train is toooo big for me, totally is 5T for train dataset although no label.</p> <pre><code>s5cmd --no-sign-request cp 's3://argoai-argoverse/av2/lidar/val/*' /home/kin/bags/av2/val\n</code></pre>"},{"location":"data/creation/#extract-point-cloud-data_1","title":"Extract Point Cloud Data","text":"<p>This time no need cpp file since argoverse have their own api things and we just need to use it. Also I write with save pcd in utils.</p> <p>Check their python api, github </p><pre><code>pip install av2\n</code></pre> <p>Please check the folder path inside the script. </p><pre><code>python3 scripts/extract_argoverse2.py\n</code></pre>"},{"location":"evaluation/","title":"Overview","text":""},{"location":"evaluation/#evaluation","title":"Evaluation","text":"<ul> <li> <p>Create the data you need for the unified benchmark. But I will recommend to download directly from link, then we don't need to read Data Creation. Most important reason is Data Creation don't include manually labeled ground truth file. But the link we provided have the human labeled ground truth.</p> <p>It will also help if you want to create your own dataset for benchmarking. Welcome to contribute your dataset to the community.</p> </li> <li> <p>Evaluate the performance of the methods. </p> </li> <li> <p>Compare the result and output the visualization automatically.</p> </li> </ul> <p>It's better to view this <code>md</code> file through outline. No need to go through all of them. \ud83d\ude03</p>"},{"location":"evaluation/#evaluation_1","title":"Evaluation","text":"<p>This part include output the quantitative table and qualitative result automatically. To be updated scripts....</p> <p>All the methods will output the clean map, so we need to extract the ground truth label from gt label based on clean map. Why we need this? Since maybe some methods will downsample in their pipeline, so we need to extract the gt label from the downsampled map.</p>"},{"location":"evaluation/#0-run-methods","title":"0. Run Methods","text":"<p>Check the <code>methods</code> folder, there is a README file to guide you how to run all the methods. </p> <p>Or check the shell script in <code>0_run_methods_all.sh</code>, run them with one command.</p> <pre><code>./scripts/sh/0_run_methods_all.sh\n</code></pre>"},{"location":"evaluation/#1-create-the-eval-data","title":"1. Create the eval data","text":"<pre><code># Check export_eval_pcd.cpp\n./export_eval_pcd [folder that you have the output pcd] [method_name_output.pcd] [min_dis to view as the same point]\n\n# example:\n./export_eval_pcd /home/kin/bags/VLP16_cone_two_people octomapfg_output.pcd 0.05\n</code></pre> <p>Or check the shell script in <code>1_export_eval_pcd.sh</code>, run them with one command.</p> <pre><code>./scripts/sh/1_export_eval_pcd.sh\n</code></pre>"},{"location":"evaluation/#2-print-the-score","title":"2. Print the score","text":"<p>Check the script and the only thing you need do is change the folder path to your data folder. And Select the methods you want to compare. Please try to open and read the script first</p> <pre><code>python3 scripts/py/eval/evaluate_all.py\n</code></pre> <p>Here is the demo output:</p> <p></p>"},{"location":"evaluation/#3-visualize-the-result","title":"3. Visualize the result","text":"<p>This jupyter-notebook scripts/py/eval/figure_plot.ipynb will help you to visualize the result and output the qualitative result directly.</p> <p></p> <p></p>"},{"location":"method/","title":"Overview","text":""},{"location":"method/#methods","title":"Methods","text":"<p>In this section we will introduce how to run the methods in the benchmark.</p> <p>Here is a demo result you can have after reading this README:</p> <p></p>"},{"location":"method/#install-build","title":"Install &amp; Build","text":"<p>Test computer and System:</p> <ul> <li>Desktop setting: i9-12900KF, 64GB RAM, Swap 90GB, 1TB SSD</li> <li>System setting: Ubuntu 20.04 [ROS noetic-full installed in system]</li> </ul>"},{"location":"method/#setup","title":"Setup","text":"<p>We show the dependencies for our octomap as an example.</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y libpcl-dev \nsudo apt install -y libgoogle-glog-dev libgflags-dev\n</code></pre>"},{"location":"method/#docker-option","title":"Docker option","text":"<p>You can use docker to build and run if you don't like trash your env and is able to run all methods in our benchmark.</p> <p>If you want to use docker, please check Dockerfile for more details.  This can also be a reference for you to install the dependencies. </p><pre><code>cd DynamicMap_Benchmark\ndocker build -t zhangkin/dynamic_map .\ndocker run -it --rm --name dynamicmap -v /home/kin/data:/home/kin/data zhangkin/dynamic_map /bin/zsh\n</code></pre> - <code>-v</code> means link your data folder to docker container, so you can use your data in docker container. <code>-v ${your_env_path}:${container_path}</code> - If it's hard to build, you can always use <code>docker pull zhangkin/dynamic_map</code> to pull the image from docker hub."},{"location":"method/#build","title":"Build","text":"<pre><code>git clone https://github.com/Kin-Zhang/octomap\ncd octomap\ncmake -B build &amp;&amp; cmake --build build\n</code></pre>"},{"location":"method/#run","title":"RUN","text":"<p>Check each methods config file in their own folder <code>config/*.yaml</code> or <code>assets/*.yaml</code> </p><pre><code>./build/${methods_name}_run ${data_path} ${config.yaml} -1\n</code></pre> <p>For example, if you want to run octomap with GF</p> <pre><code>./build/octomap_run /home/kin/data/00 assets/config_fg.yaml -1\n</code></pre> <p>Then you can get a time table with clean map result from Octomap w GF like top image shows. You can also output the voxel map by changing the config file.</p> <p></p> <p>Other methods like DUFOMap (by cd into the benchmark submodule or clone alone), you can run like this:</p> <pre><code>git clone https://github.com/Kin-Zhang/dufomap.git\ncmake -B build -D CMAKE_CXX_COMPILER=g++-10 &amp;&amp; cmake --build build\n./build/dufomap_run /home/kin/data/semindoor assets/config.toml\n</code></pre> <p></p> <p>All Running commands, <code>-1</code> means all pcd files in the <code>pcd</code> folder, if you only want to run <code>10</code> frames change to <code>10</code>.</p> <pre><code>./build/octomap_run ${data_path} ${config.yaml} -1\n./build/dufomap_run ${data_path} ${config.toml}\n\n# beautymap\npython main.py --data_dir data/00 --dis_range 40 --xy_resolution 1 --h_res 0.5\n\n# deflow\npython main.py checkpoint=/home/kin/deflow_best.ckpt dataset_path=/home/kin/data/00\n</code></pre> <p>Then maybe you would like to have quantitative and qualitative result, check evaluation part.</p>"},{"location":"method/dufomap/","title":"DUFOMap","text":""},{"location":"method/dufomap/#dufomap","title":"DUFOMap","text":"<p>Here is a quick blog post about the DUFOMap paper. I will soon update more details here.</p>"}]}