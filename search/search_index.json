{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the Dynamic Map Benchmark Wiki Page!</p> <p>You can always press <code>F</code> or top right search bar to search for specific topics.</p> CHANGELOG: <ul> <li>2024/06/25: Qingwen is starting to work on the wiki page.  </li> <li>2024/04/29 BeautyMap is accepted by RA-L'24. Updated benchmark: BeautyMap and DeFlow submodule instruction in the benchmark. Added the first data-driven method DeFlow into our benchmark. Feel free to check.</li> <li>2024/04/18 DUFOMap is accepted by RA-L'24. Updated benchmark: DUFOMap and dynablox submodule instruction in the benchmark. Two datasets w/o gt for demo are added in the download link. Feel free to check.</li> <li>2024/03/08 Fix statements on our ITSC'23 paper: KITTI sequences pose are also from SemanticKITTI which used SuMa. In the DUFOMap paper Section V-C, Table III, we present the dynamic removal result on different pose sources. Check discussion in DUFOMap paper if you are interested.</li> <li>2023/06/13 The benchmark paper Accepted by ITSC 2023 and release five methods (Octomap, Octomap w GF, ERASOR, Removert) and three datasets (01, 05, av2, semindoor) in benchmark paper.</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Task: Detect and Remove dynamic points from Point Cloud Maps.</p> <p></p> <p>Here is a figure that illustrate of ghost points resulting from dynamic objects in KITTI sequence 7.  The yellow points to the right represent points labeled as belonging to dynamic objects in the dataset.  These ghost points negatively affect downstream tasks and overall point cloud quality</p>"},{"location":"#good-to-start-from-here","title":"\ud83c\udfd8\ufe0f Good to start from here","text":"<ul> <li> <p>What kind of data format we use? </p> <p>PCD files (pose information saved in <code>VIEWPOINT</code> header). Read Data Section</p> </li> <li> <p>How to evaluate the performance of a method?</p> <p>Two python scripts. Read Evaluation Section</p> </li> <li> <p>How to run a benchmark method on my data?</p> <p>Format your data and run the method. Read Create data and Run method</p> </li> </ul>"},{"location":"#methods-we-included","title":"\ud83c\udf81 Methods we included","text":"<p>Online (w/o prior map):</p> <ul> <li> DUFOMap (Ours \ud83d\ude80): RAL'24, Benchmark Instruction</li> <li> Octomap w GF (Ours \ud83d\ude80): ITSC'23, Benchmark improvement ITSC 2023</li> <li> dynablox: RAL'23 official link, Benchmark Adaptation </li> <li> Octomap: ICRA'10 &amp; AR'13 official link, Benchmark implementation</li> </ul> <p>Learning-based (data-driven) (w pretrain-weights provided):</p> <ul> <li> DeFlow (Ours \ud83d\ude80): ICRA'24, Benchmark Adaptation</li> </ul> <p>Offline (need prior map).</p> <ul> <li> BeautyMap (Ours \ud83d\ude80): RAL'24, Official Code</li> <li> ERASOR: RAL'21 official link, benchmark implementation</li> <li> Removert: IROS 2020 official link, benchmark implementation</li> </ul> <p>Please note that we provided the comparison methods also but modified a little bit for us to run the experiments quickly, but no modified on their methods' core. Please check the LICENSE of each method in their official link before using it.</p> <p>You will find all methods in this benchmark under <code>methods</code> folder. So that you can easily reproduce the experiments. Or click here to check our score screenshot directly. </p> <p>Last but not least, feel free to pull request if you want to add more methods. Welcome!</p>"},{"location":"#acknowledgements","title":"\ud83d\udc96 Acknowledgements","text":"<p>This benchmark implementation is based on codes from several repositories as we mentioned in the beginning. Thanks for these authors who kindly open-sourcing their work to the community. Please see our paper reference section to get more information.</p> <p>Thanks to HKUST Ramlab's members: Bowen Yang, Lu Gan, Mingkai Tang, and Yingbing Chen, who help collect additional datasets.</p> <p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation</p>"},{"location":"#cite-our-papers","title":"Cite Our Papers","text":"<p>Please cite our works if you find these useful for your research:</p> <pre><code>@inproceedings{zhang2023benchmark,\n  author={Zhang, Qingwen and Duberg, Daniel and Geng, Ruoyu and Jia, Mingkai and Wang, Lujia and Jensfelt, Patric},\n  booktitle={IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, \n  title={A Dynamic Points Removal Benchmark in Point Cloud Maps}, \n  year={2023},\n  pages={608-614},\n  doi={10.1109/ITSC57777.2023.10422094}\n}\n@article{jia2024beautymap,\n  author={Jia, Mingkai and Zhang, Qingwen and Yang, Bowen and Wu, Jin and Liu, Ming and Jensfelt, Patric},\n  journal={IEEE Robotics and Automation Letters}, \n  title={BeautyMap: Binary-Encoded Adaptable Ground Matrix for Dynamic Points Removal in Global Maps}, \n  year={2024},\n  volume={},\n  number={},\n  pages={1-8},\n  doi={10.1109/LRA.2024.3402625}\n}\n@article{daniel2024dufomap,\n  author={Duberg, Daniel and Zhang, Qingwen and Jia, Mingkai and Jensfelt, Patric},\n  journal={IEEE Robotics and Automation Letters}, \n  title={{DUFOMap}: Efficient Dynamic Awareness Mapping}, \n  year={2024},\n  volume={9},\n  number={6},\n  pages={5038-5045},\n  doi={10.1109/LRA.2024.3387658}\n}\n</code></pre>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data","title":"Data","text":"<p>In this section, we will introduce the data format we use in the benchmark, and how to prepare the data (public datasets or collected by ourselves) for the benchmark.</p>"},{"location":"data/#format","title":"Format","text":"<p>We saved all our data into PCD files, first let me introduce the PCD file format:</p> <p>The important two for us are <code>VIEWPOINT</code>, <code>POINTS</code> and <code>DATA</code>:</p> <ul> <li> <p>VIEWPOINT - specifies an acquisition viewpoint for the points in the dataset. This could potentially be later on used for building transforms between different coordinate systems, or for aiding with features such as surface normals, that need a consistent orientation.</p> <p>The viewpoint information is specified as a translation (tx ty tz) + quaternion (qw qx qy qz). The default value is:</p> <pre><code>VIEWPOINT 0 0 0 1 0 0 0\n</code></pre> </li> <li> <p>POINTS - specifies the number of points in the dataset.</p> </li> <li> <p>DATA - specifies the data type that the point cloud data is stored in. As of version 0.7, three data types are supported: ascii, binary, and binary_compressed. We saved as binary for faster reading and writing.</p> </li> </ul>"},{"location":"data/#example","title":"Example","text":"<pre><code># .PCD v0.7 - Point Cloud Data file format\nVERSION 0.7\nFIELDS x y z intensity\nSIZE 4 4 4 4\nTYPE F F F F\nCOUNT 1 1 1 1\nWIDTH 125883\nHEIGHT 1\nVIEWPOINT -15.6504 17.981 -0.934952 0.882959 -0.0239536 -0.0058903 -0.468802\nPOINTS 125883\nDATA binary\n</code></pre> <p>In this <code>004390.pcd</code> we have 125883 points, and the pose (sensor center) of this frame is: <code>-15.6504 17.981 -0.934952 0.882959 -0.0239536 -0.0058903 -0.468802</code>. All points are already transformed to the world frame.</p>"},{"location":"data/#download-benchmark-data","title":"Download benchmark data","text":"<p>We already processed the data in the benchmark, you can download the data from the following links:</p> Dataset Description Sensor Type Total Frame Number Size KITTI sequence 00 in a small town with few dynamics (including one pedestrian around) VLP-64 141 384.8 MB KITTI sequence 05 in a small town straight way, one higher car, the benchmarking paper cover image from this sequence. VLP-64 321 864.0 MB Argoverse2 in a big city, crowded and tall buildings (including cyclists, vehicles, people walking near the building etc. 2 x VLP-32 575 1.3 GB KTH campus (no gt) Collected by us (Thien-Minh) on the KTH campus. Lots of people move around on the campus. Leica RTC360 18 256.4 MB Semi-indoor Collected by us, running on a small 1x2 vehicle with two people walking around the platform. VLP-16 960 620.8 MB Twofloor (no gt) Collected by us (Bowen Yang) in a quadruped robot. A two-floor structure environment with one pedestrian around. Livox-mid 360 3305 725.1 MB <p>Download command: </p><pre><code>wget https://zenodo.org/api/records/10886629/files-archive.zip\n\n# or download each sequence separately\nwget https://zenodo.org/records/10886629/files/00.zip\nwget https://zenodo.org/records/10886629/files/05.zip\nwget https://zenodo.org/records/10886629/files/av2.zip\nwget https://zenodo.org/records/10886629/files/kthcampus.zip\nwget https://zenodo.org/records/10886629/files/semindoor.zip\nwget https://zenodo.org/records/10886629/files/twofloor.zip\n</code></pre>"},{"location":"data/#create-by-yourself","title":"Create by yourself","text":"<p>If you want to process more data, you can follow the instructions below. (</p> <p>Note</p> <p>Feel free to skip this section if you only want to use the benchmark data.    </p>"},{"location":"data/#custom-data","title":"Custom Data","text":"<p>For our custom dataset, we normally record the pointcloud with rosbag, and then running some slam methods to get the pose. If you don't have clue to use the slam package, check simple_ndt_slam repo the only dependence you need in the repo is ROS. If you don't have ROS/Ubuntu, you can directly use the <code>docker</code> to run.</p> <p>Then, directly export rosbag file [which have pose/tf and pointcloud topic] to pcd we want, after your run with <code>simple_ndt_slam</code> check your result rosbag file by <code>rosbag info</code>, here is example output:</p> <pre><code>\u279c  bags rosbag info res_semi_indoor_data.bag\npath:        res_semi_indoor_data.bag\nversion:     2.0\nduration:    1:47s (107s)\nstart:       Apr 28 2023 11:11:26.79 (1682673086.79)\nend:         Apr 28 2023 11:13:14.35 (1682673194.35)\nsize:        810.8 MB\nmessages:    4803\ncompression: none [961/961 chunks]\ntypes:       nav_msgs/Odometry       [cd5e73d190d741a2f92e81eda573aca7]\n             sensor_msgs/PointCloud2 [1158d486dd51d683ce2f1be655c3c181]\n             tf2_msgs/TFMessage      [94810edda583a504dfda3829e70d7eec]\ntopics:      /auto_odom       960 msgs    : nav_msgs/Odometry      \n             /repub_points    960 msgs    : sensor_msgs/PointCloud2\n             /tf             2883 msgs    : tf2_msgs/TFMessage\n</code></pre> <p>Then use the scripts I provided in <code>simple_ndt_slam</code> to extract the pcd data to unified format here.</p> <pre><code>roscore # since need read rosbag through scripts\n\n./simple_ndt_slam/tools/build/bag2pcd_tf /home/kin/bags/res_semi_indoor_data.bag /home/kin/data/semindoor /repub_points map 1 # 1 for save raw map also since some methods need use it in the framework.\n</code></pre>"},{"location":"data/#kitti-dataset","title":"KITTI Dataset","text":"<p>Official data format Download link</p>"},{"location":"data/#extract-point-cloud-data","title":"Extract Point Cloud Data","text":"<p>extract the semantic-kitti dataset from the raw data, when you download the original SemanticKITTI dataset, you will get a folder like this: </p><pre><code>\u279c  SemanticKitti tree -L 2\n.\n\u251c\u2500\u2500 data_odometry_calib\n\u2502   \u2514\u2500\u2500 dataset\n\u2502       \u2514\u2500\u2500 sequences\n\u251c\u2500\u2500 data_odometry_labels\n\u2502   \u251c\u2500\u2500 dataset\n\u2502   \u2502   \u2514\u2500\u2500 sequences\n\u2502   \u2514\u2500\u2500 README\n\u251c\u2500\u2500 data_odometry_velodyne\n\u2502   \u2514\u2500\u2500 dataset\n\u2502       \u2514\u2500\u2500 sequences\n</code></pre> <p>After downloading the official dataset, Run the script like follows: </p><pre><code>python3 scripts/data/extract_semkitti.py --original_path /home/kin/data/KITTI/SemanticKitti --save_data_folder /home/kin/data/DynamicMap --gt_cloud True --sequence \"'00'\" \n</code></pre> <p>Note!! </p> <ol> <li> <p>SemanticKITTI pose file is not ground truth pose but run SuMa, more discussion and different can be found here in semantic-kitti-api/issues/140. We have extra different odometry pose result in DUFOMap paper, Sec V-C, Table III, based on scripts/py/data/extract_diff_pose.py</p> </li> <li> <p>You can get the sensor pose in the PCD <code>VIEWPOINT</code> Field, so you don't need pose file etc.     If you are using CloudCompare to view, drag all pcd files to windows, you will have the correct whole map view.     (NOTE Since we already transform to world frame CloudCompare in 2.11 version will looks correct map     but version to 2.12+ will have double effect on VIEWPOINT Field [you can comment the transform line if you don't like that.] )</p> <p>Example here: </p> </li> <li> <p>View the ground truth in CloudCompare, intensity=1 means dynamic which are red points in images:</p> <p></p> </li> <li> <p>2024/03/27 Updated version limit the range because we find the ground truth label is not correct in the far range, so we limit the range to 50m. You can change the range in the script.</p> <p></p> </li> </ol>"},{"location":"data/#argoverse-20-dataset","title":"Argoverse 2.0 Dataset","text":"<p>I manually labeled dynamic and static in one sequence folder name: <code>07YOTznatmYypvQYpzviEcU3yGPsyaGg__Spring_2020</code> , you have to go for website to downloaded this ground truth PCD.</p>"},{"location":"data/#download","title":"Download","text":"<p>Check this issue: https://github.com/argoverse/av2-api/issues/161</p> <p>Installing s5cmd</p> <pre><code>#!/usr/bin/env bash\n\nexport INSTALL_DIR=$HOME/.local/bin\nexport PATH=$PATH:$INSTALL_DIR\nexport S5CMD_URI=https://github.com/peak/s5cmd/releases/download/v1.4.0/s5cmd_1.4.0_$(uname | sed 's/Darwin/macOS/g')-64bit.tar.gz\n\nmkdir -p $INSTALL_DIR\ncurl -sL $S5CMD_URI | tar -C $INSTALL_DIR -xvzf - s5cmd\n</code></pre> <p>Download the val dataset since train is toooo big for me, totally is 5T for train dataset although no label.</p> <pre><code>s5cmd --no-sign-request cp 's3://argoai-argoverse/av2/lidar/val/*' /home/kin/bags/av2/val\n</code></pre>"},{"location":"data/#extract-point-cloud-data_1","title":"Extract Point Cloud Data","text":"<p>This time no need cpp file since argoverse have their own api things and we just need to use it. Also I write with save pcd in utils.</p> <p>Check their python api, github </p><pre><code>pip install av2\n</code></pre> <p>Please check the folder path inside the script. </p><pre><code>python3 scripts/extract_argoverse2.py\n</code></pre>"},{"location":"evaluation/","title":"Evaluation","text":""},{"location":"evaluation/#evaluation","title":"Evaluation","text":"<ul> <li> <p>Create the data you need for the unified benchmark. But I will recommend to download directly from link, then we don't need to read Data Creation. Most important reason is Data Creation don't include manually labeled ground truth file. But the link we provided have the human labeled ground truth.</p> <p>It will also help if you want to create your own dataset for benchmarking. Welcome to contribute your dataset to the community.</p> </li> <li> <p>Evaluate the performance of the methods. </p> </li> <li> <p>Compare the result and output the visualization automatically.</p> </li> </ul> <p>It's better to view this <code>md</code> file through outline. No need to go through all of them. \ud83d\ude03</p>"},{"location":"evaluation/#evaluation_1","title":"Evaluation","text":"<p>This part include output the quantitative table and qualitative result automatically. To be updated scripts....</p> <p>All the methods will output the clean map, so we need to extract the ground truth label from gt label based on clean map. Why we need this? Since maybe some methods will downsample in their pipeline, so we need to extract the gt label from the downsampled map.</p>"},{"location":"evaluation/#0-run-methods","title":"0. Run Methods","text":"<p>Check the <code>methods</code> folder, there is a README file to guide you how to run all the methods. </p> <p>Or check the shell script in <code>0_run_methods_all.sh</code>, run them with one command.</p> <pre><code>./scripts/sh/0_run_methods_all.sh\n</code></pre>"},{"location":"evaluation/#1-create-the-eval-data","title":"1. Create the eval data","text":"<pre><code># Check export_eval_pcd.cpp\n./export_eval_pcd [folder that you have the output pcd] [method_name_output.pcd] [min_dis to view as the same point]\n\n# example:\n./export_eval_pcd /home/kin/bags/VLP16_cone_two_people octomapfg_output.pcd 0.05\n</code></pre> <p>Or check the shell script in <code>1_export_eval_pcd.sh</code>, run them with one command.</p> <pre><code>./scripts/sh/1_export_eval_pcd.sh\n</code></pre>"},{"location":"evaluation/#2-print-the-score","title":"2. Print the score","text":"<p>Check the script and the only thing you need do is change the folder path to your data folder. And Select the methods you want to compare. Please try to open and read the script first</p> <pre><code>python3 scripts/py/eval/evaluate_all.py\n</code></pre> <p>Here is the demo output:</p> <p></p>"},{"location":"method/","title":"Method","text":""},{"location":"method/#methods","title":"Methods","text":"<p>Demo Image, results you can have after reading this README:</p> <p></p> <p>All of them have same dependencies [PCL, Glog, yaml-cpp], we will show how to install and build:</p>"},{"location":"method/#install-build","title":"Install &amp; Build","text":"<p>Test computer and System:</p> <ul> <li>Desktop setting: i9-12900KF, 64GB RAM, Swap 90GB, 1TB SSD</li> <li>System setting: Ubuntu 20.04 [ROS noetic-full installed in system]</li> </ul> <p>Dependencies listed following if you want to install them manually, or you can use docker to build and run if you don't like trash your env.</p>"},{"location":"method/#docker","title":"Docker","text":"<p>If you want to use docker, please check Dockerfile for more details.  This can also be a reference for you to install the dependencies. </p><pre><code>cd DynamicMap_Benchmark\ndocker build -t zhangkin/dynamic_map .\ndocker run -it --rm --name dynamicmap -v /home/kin/data:/home/kin/data zhangkin/dynamic_map /bin/zsh\n</code></pre> - <code>-v</code> means link your data folder to docker container, so you can use your data in docker container. <code>-v ${your_env_path}:${container_path}</code> - If it's hard to build, you can always use <code>docker pull zhangkin/dynamic_map</code> to pull the image from docker hub."},{"location":"method/#pcl-opencv","title":"PCL / OpenCV","text":"<p>Normally, you will directly have PCL and OpenCV library if you installed ROS-full in your computer.  OpenCV is required by Removert only, PCL is required by Benchmark.</p> <p>Reminder for ubuntu 20.04 may occur this error: </p><pre><code>fatal error: opencv2/cv.h: No such file or directory\n</code></pre> ln from opencv4 to opencv2 <pre><code>sudo ln -s /usr/include/opencv4/opencv2 /usr/include/opencv2\n</code></pre>"},{"location":"method/#glog-gflag-for-print","title":"glog gflag [for print]","text":"<p>or you can install through <code>sudo apt install</code> </p><pre><code>sh -c \"$(wget -O- https://raw.githubusercontent.com/Kin-Zhang/Kin-Zhang/main/Dockerfiles/latest_glog_gflag.sh)\"\n</code></pre>"},{"location":"method/#yaml-cpp-for-config","title":"yaml-cpp [for config]","text":"<p>Please set the FLAG, check this issue if you want to know more: https://github.com/jbeder/yaml-cpp/issues/682, TOOD inside the CMakeLists.txt</p> <p>If you install in Ubuntu 22.04, please check this commit: https://github.com/jbeder/yaml-cpp/commit/c86a9e424c5ee48e04e0412e9edf44f758e38fb9 which is the version could build in 22.04</p> <pre><code>cd ${Tmp_folder}\ngit clone https://github.com/jbeder/yaml-cpp.git &amp;&amp; cd yaml-cpp\nenv CFLAGS='-fPIC' CXXFLAGS='-fPIC' cmake -Bbuild\ncmake --build build --config Release\nsudo cmake --build build --config Release --target install\n</code></pre>"},{"location":"method/#build","title":"Build","text":"<pre><code>cd ${methods you want}\ncmake -B build &amp;&amp; cmake --build build\n</code></pre>"},{"location":"method/#run","title":"RUN","text":"<p>Check each methods config file in their own folder <code>config/*.yaml</code> or <code>assets/*.yaml</code> </p><pre><code>./build/${methods_name}_run ${data_path} ${config.yaml} -1\n</code></pre> <p>For example, if you want to run octomap with GF</p> <pre><code>./build/octomap_run /home/kin/data/00 assets/config_fg.yaml -1\n</code></pre> <p>Then you can get a time table with clean map result from Octomap w GF like top image shows. Or ERASOR on semindoor dataset:</p> <pre><code>./build/dufomap_run /home/kin/data/semindoor assets/config.toml\n</code></pre> <p>All Running commands, <code>-1</code> means all pcd files in the <code>pcd</code> folder, if you only want to run <code>10</code> frames change to <code>10</code>.</p> <pre><code>./build/octomap_run ${data_path} ${config.yaml} -1\n./build/dufomap_run ${data_path} ${config.toml}\n\n# beautymap\npython main.py --data_dir data/00 --dis_range 40 --xy_resolution 1 --h_res 0.5\n\n# deflow\npython main.py checkpoint=/home/kin/deflow_best.ckpt dataset_path=/home/kin/data/00\n</code></pre> <p>Then maybe you would like to have quantitative and qualitative result, check scripts/py/eval.</p>"}]}